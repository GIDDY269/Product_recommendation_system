{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA INGESTION NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1732887267906
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-07 12:29:45,009 ] 161 numexpr.utils - INFO - NumExpr defaulting to 4 threads.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/azureuser/cloudfiles/code/Users/oviemunooboro/Product_recommendation_system')\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from src.utils.commons import read_yaml,create_directories\n",
    "from src.cloud_storage.azure_blob_storage import AzureDatastore\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from src.logger import logging\n",
    "import pandas  as pd\n",
    "from src.constants import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "from src.utils.commons import unzip_files\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1732887278115
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataIngestionConfig:\n",
    "    root_dir : Path\n",
    "    local_path : Path\n",
    "    target_path : str\n",
    "    registered_name : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1732887291096
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,config_filepath=CONFIG_FILE_PATH,\n",
    "                 params_filepath=PARAMS_FILE_PATH,\n",
    "                 schema_filepath=SCHEMA_FILE_PATH):\n",
    "        '''\n",
    "        Initiating the configuration manager\n",
    "        '''\n",
    "        # read YAML configuration files to initatiate configuration parameters\n",
    "        self.config = read_yaml(str(config_filepath))\n",
    "        self.params = read_yaml(str(params_filepath))\n",
    "        self.schema = read_yaml(str(schema_filepath))\n",
    "\n",
    "        logging.info(f'configuration: {self.config}')\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    #configuration manager\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        '''\n",
    "        Function to get configuration settings\n",
    "        '''\n",
    "\n",
    "        # read data ingestion configuration section from config.yaml\n",
    "        config = self.config.DATA_INGESTION\n",
    "\n",
    "        # create a new artifacts folder to store ingested data if doesn't exist already \n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        # create and return dataingestion configuration object\n",
    "\n",
    "        config_object = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            local_path= config.local_path,\n",
    "            target_path=config.target_path,\n",
    "            registered_name= config.registered_name\n",
    "        )\n",
    "\n",
    "        return config_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1732887300975
    }
   },
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self,config:DataIngestionConfig):\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "    def initiate_data_ingestion(self):\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_timestamp = datetime.now().strftime(\"%m_%d_%Y %H:%M:%S\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            logging.info(f'Unziping file from {self.config.local_path} into {self.config.root_dir} ')\n",
    "            unzip_folder = os.path.join(self.config.root_dir,'unzipped_data') # folder to extract data to\n",
    "\n",
    "            if not os.path.exists(unzip_folder) or not os.listdir(unzip_folder) :\n",
    "                unzip_files(self.config.local_path,unzip_folder)\n",
    "                logging.info(f'Data from {self.config.local_path} unzippeded into {unzip_folder}')\n",
    "            else:\n",
    "                logging.info(f'data from {self.config.local_path} already unzipped')\n",
    "                pass\n",
    "\n",
    "\n",
    "            Azure_ws = AzureDatastore()\n",
    "\n",
    "            Azure_ws.load_local_data_to_Azure_datastore(\n",
    "                src_dir = unzip_folder,\n",
    "                target_path = self.config.target_path,\n",
    "                registered_name = self.config.registered_name\n",
    "            )\n",
    "\n",
    "            number_of_files = len(os.listdir(unzip_folder))\n",
    "            logging.info(f'Number of ingested files : {number_of_files}')\n",
    "\n",
    "\n",
    "            # save metadata\n",
    "            end_time = time.time()\n",
    "            end_timestamp = datetime.now().strftime(\"%m_%d_%Y %H:%M:%S\")\n",
    "            duration = end_time - start_time\n",
    "            metadata = {\n",
    "                'start_time' : start_timestamp,\n",
    "                'end_time' : end_timestamp,\n",
    "                'duration' : duration,\n",
    "                'Number of files loaded to workspace' : number_of_files,\n",
    "                'data_source' : self.config.local_path,\n",
    "                'target_path_from_datastore' : self.config.target_path,\n",
    "                'registered_name' : self.config.registered_name,\n",
    "                'Project name' : 'Data ingestion'\n",
    "            }\n",
    "\n",
    "            # download data\n",
    "            logging.info('downloading data from datastore')\n",
    "            Azure_ws.download_from_datastore(\n",
    "                registered_name=self.config.registered_name,\n",
    "                target_path=self.config.target_path\n",
    "            )\n",
    "            metadata_path = os.path.join(unzip_folder,'metadata.json')\n",
    "            pd.Series(metadata).to_json(metadata_path)\n",
    "    \n",
    "            logging.info(f'saved ingestion pipeline metadata into {metadata_path}')\n",
    "            \n",
    "        except ClientError as e:             \n",
    "            logging.info(f'error occured {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1732887778038
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-07 12:29:57,209 ] 38 root - INFO - Yaml file:  config/config.yaml loaded suscessfully\n",
      "[2024-12-07 12:29:57,236 ] 38 root - INFO - Yaml file:  params.yaml loaded suscessfully\n",
      "[2024-12-07 12:29:57,261 ] 38 root - INFO - Yaml file:  schema.yaml loaded suscessfully\n",
      "[2024-12-07 12:29:57,263 ] 13 root - INFO - configuration: {'DATA_INGESTION': {'root_dir': 'Artifacts', 'local_path': '/home/azureuser/cloudfiles/code/Users/oviemunooboro/Product_recommendation_system/Artifacts/customer_interactions.zip', 'target_path': 'Artifacts/Unzipped_data', 'registered_name': 'customer_interaction_dataset'}, 'DATA_VALIDATION': {'root_dir': 'Artifacts/data_validation', 'data_source': 'Artifacts/Raw_ingested_data/', 'status_file': 'Artifacts/data_validation/status.json', 'critical_columns': ['event_time', 'event_type', 'product_id', 'category_id', 'category_code', 'brand', 'price', 'user_id', 'user_session']}}\n",
      "[2024-12-07 12:29:57,267 ] 61 root - INFO - File directory create at : Artifacts\n",
      "[2024-12-07 12:29:57,267 ] 13 root - INFO - Unziping file from /home/azureuser/cloudfiles/code/Users/oviemunooboro/Product_recommendation_system/Artifacts/customer_interactions.zip into Artifacts \n",
      "[2024-12-07 12:29:57,285 ] 61 root - INFO - File directory create at : Artifacts/unzipped_data\n",
      "[2024-12-07 12:29:57,286 ] 188 root - INFO - Unzipping data frrom /home/azureuser/cloudfiles/code/Users/oviemunooboro/Product_recommendation_system/Artifacts/customer_interactions.zip to load into Artifacts/unzipped_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-07 12:30:21,149 ] 195 root - INFO - Files extracted successfully to Artifacts/unzipped_data\n",
      "[2024-12-07 12:30:21,150 ] 18 root - INFO - Data from /home/azureuser/cloudfiles/code/Users/oviemunooboro/Product_recommendation_system/Artifacts/customer_interactions.zip unzippeded into Artifacts/unzipped_data\n",
      "[2024-12-07 12:30:21,151 ] 16 root - INFO - configuring workspace\n",
      "[2024-12-07 12:30:21,184 ] 291 azureml.core.workspace - INFO - Found the config file in: /config.json\n",
      "[2024-12-07 12:30:22,089 ] 991 azureml.data.datastore_client - INFO - <azureml.core.authentication.InteractiveLoginAuthentication object at 0x7f86ea6536d0>\n",
      "[2024-12-07 12:30:23,586 ] 37 root - INFO - Uploading data from local machine into blob storage using datastore\n",
      "[2024-12-07 12:30:23,587 ] 89 azureml.data._dataset_deprecation - WARNING - \"Datastore.upload\" is deprecated after version 1.0.69. Please use \"Dataset.File.upload_directory\" to upload your files             from a local directory and create FileDataset in single method call. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n",
      "[2024-12-07 12:30:23,588 ] 923 azureml.data.azure_storage_datastore - INFO - Called AzureBlobDatastore.upload\n",
      "Uploading an estimated of 5 files\n",
      "[2024-12-07 12:30:23,802 ] 372 azureml.data.azure_storage_datastore - INFO - Uploading an estimated of 5 files\n",
      "Uploading Artifacts/unzipped_data/2019-Dec.csv\n",
      "[2024-12-07 12:30:40,084 ] 372 azureml.data.azure_storage_datastore.task_upload_Artifacts/Unzipped_data/2019-Dec.csv - INFO - Uploading Artifacts/unzipped_data/2019-Dec.csv\n",
      "Uploaded Artifacts/unzipped_data/2019-Dec.csv, 1 files out of an estimated total of 5\n",
      "[2024-12-07 12:30:40,085 ] 372 azureml.data.azure_storage_datastore.task_upload_Artifacts/Unzipped_data/2019-Dec.csv - INFO - Uploaded Artifacts/unzipped_data/2019-Dec.csv, 1 files out of an estimated total of 5\n",
      "Uploading Artifacts/unzipped_data/2019-Oct.csv\n",
      "[2024-12-07 12:30:41,343 ] 372 azureml.data.azure_storage_datastore.task_upload_Artifacts/Unzipped_data/2019-Oct.csv - INFO - Uploading Artifacts/unzipped_data/2019-Oct.csv\n",
      "Uploaded Artifacts/unzipped_data/2019-Oct.csv, 2 files out of an estimated total of 5\n",
      "[2024-12-07 12:30:41,344 ] 372 azureml.data.azure_storage_datastore.task_upload_Artifacts/Unzipped_data/2019-Oct.csv - INFO - Uploaded Artifacts/unzipped_data/2019-Oct.csv, 2 files out of an estimated total of 5\n",
      "Uploading Artifacts/unzipped_data/2020-Jan.csv\n",
      "[2024-12-07 12:30:42,598 ] 372 azureml.data.azure_storage_datastore.task_upload_Artifacts/Unzipped_data/2020-Jan.csv - INFO - Uploading Artifacts/unzipped_data/2020-Jan.csv\n",
      "Uploaded Artifacts/unzipped_data/2020-Jan.csv, 3 files out of an estimated total of 5\n",
      "[2024-12-07 12:30:42,599 ] 372 azureml.data.azure_storage_datastore.task_upload_Artifacts/Unzipped_data/2020-Jan.csv - INFO - Uploaded Artifacts/unzipped_data/2020-Jan.csv, 3 files out of an estimated total of 5\n",
      "Uploading Artifacts/unzipped_data/2020-Feb.csv\n",
      "[2024-12-07 12:30:43,351 ] 372 azureml.data.azure_storage_datastore.task_upload_Artifacts/Unzipped_data/2020-Feb.csv - INFO - Uploading Artifacts/unzipped_data/2020-Feb.csv\n",
      "Uploaded Artifacts/unzipped_data/2020-Feb.csv, 4 files out of an estimated total of 5\n",
      "[2024-12-07 12:30:43,352 ] 372 azureml.data.azure_storage_datastore.task_upload_Artifacts/Unzipped_data/2020-Feb.csv - INFO - Uploaded Artifacts/unzipped_data/2020-Feb.csv, 4 files out of an estimated total of 5\n",
      "Uploading Artifacts/unzipped_data/2019-Nov.csv\n",
      "[2024-12-07 12:30:43,855 ] 372 azureml.data.azure_storage_datastore.task_upload_Artifacts/Unzipped_data/2019-Nov.csv - INFO - Uploading Artifacts/unzipped_data/2019-Nov.csv\n",
      "Uploaded Artifacts/unzipped_data/2019-Nov.csv, 5 files out of an estimated total of 5\n",
      "[2024-12-07 12:30:43,856 ] 372 azureml.data.azure_storage_datastore.task_upload_Artifacts/Unzipped_data/2019-Nov.csv - INFO - Uploaded Artifacts/unzipped_data/2019-Nov.csv, 5 files out of an estimated total of 5\n",
      "Uploaded 5 files\n",
      "[2024-12-07 12:30:43,857 ] 372 azureml.data.azure_storage_datastore - INFO - Uploaded 5 files\n",
      "[2024-12-07 12:30:43,857 ] 941 azureml.data.azure_storage_datastore - INFO - Finished AzureBlobDatastore.upload with count=5.\n",
      "[2024-12-07 12:30:43,859 ] 44 root - INFO - Files uploaded into blob store sucessesfully\n",
      "[2024-12-07 12:30:43,860 ] 48 root - INFO - Registering dataset into workspace\n",
      "[2024-12-07 12:30:43,861 ] 52 root - INFO - File path for dataset: Artifacts/Unzipped_data/*.csv\n",
      "[2024-12-07 12:30:44,659 ] 55 root - INFO - Registering dataset with name: customer_interaction_dataset\n",
      "[2024-12-07 12:30:45,089 ] 67 root - INFO - Error occured : could not upload data to blob storage: UserErrorException:\n",
      "\tMessage: There is already a dataset registered under name \"customer_interaction_dataset\". Specify `create_new_version=True` to register the dataset as a new version. Use `update`, `add_tags`, or `remove_tags` to change only the description or tags.\n",
      "\tInnerException None\n",
      "\tErrorResponse \n",
      "{\n",
      "    \"error\": {\n",
      "        \"code\": \"UserError\",\n",
      "        \"message\": \"There is already a dataset registered under name \\\"customer_interaction_dataset\\\". Specify `create_new_version=True` to register the dataset as a new version. Use `update`, `add_tags`, or `remove_tags` to change only the description or tags.\"\n",
      "    }\n",
      "}\n",
      "[2024-12-07 12:30:45,106 ] 33 root - INFO - Number of ingested files : 5\n",
      "[2024-12-07 12:30:45,107 ] 52 root - INFO - downloading data from datastore\n",
      "{'infer_column_types': 'False', 'activity': 'download'}\n",
      "{'infer_column_types': 'False', 'activity': 'download', 'activityApp': 'FileDataset'}\n",
      "[2024-12-07 12:31:00,825 ] 60 root - INFO - saved ingestion pipeline metadata into Artifacts/unzipped_data/metadata.json\n"
     ]
    }
   ],
   "source": [
    "manager = ConfigurationManager()\n",
    "data_ingestion_config = manager.get_data_ingestion_config()\n",
    "\n",
    "\n",
    "data = DataIngestion(config=data_ingestion_config)\n",
    "filepath = data.initiate_data_ingestion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
