{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA INGESTION NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1732887267906
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/giddy/Documents/RECOMMENDATION_SYSTEM')\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from src.utils.commons import read_yaml,create_directories\n",
    "from src.cloud_storage.S3_object_store import S3Client\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from src.logger import logging\n",
    "import pandas  as pd\n",
    "from src.constants import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "from src.utils.commons import unzip_files\n",
    "from glob import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1732887278115
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataIngestionConfig:\n",
    "    root_dir : Path\n",
    "    bucket : str\n",
    "    local_path : Path\n",
    "    target_path : str\n",
    "    object_name_prefix : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1732887291096
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,config_filepath=CONFIG_FILE_PATH,\n",
    "                 params_filepath=PARAMS_FILE_PATH,\n",
    "                 schema_filepath=SCHEMA_FILE_PATH):\n",
    "        '''\n",
    "        Initiating the configuration manager\n",
    "        '''\n",
    "        # read YAML configuration files to initatiate configuration parameters\n",
    "        self.config = read_yaml(str(config_filepath))\n",
    "        self.params = read_yaml(str(params_filepath))\n",
    "        self.schema = read_yaml(str(schema_filepath))\n",
    "\n",
    "        logging.info(f'configuration: {self.config}')\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    #configuration manager\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        '''\n",
    "        Function to get configuration settings\n",
    "        '''\n",
    "\n",
    "        # read data ingestion configuration section from config.yaml\n",
    "        config = self.config.DATA_INGESTION\n",
    "\n",
    "        # create a new artifacts folder to store ingested data if doesn't exist already \n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        # create and return dataingestion configuration object\n",
    "\n",
    "        config_object = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            local_path= config.local_path,\n",
    "            target_path=config.target_path,\n",
    "            bucket = config.bucket,\n",
    "            object_name_prefix= config.object_name_prefix\n",
    "           \n",
    "        )\n",
    "\n",
    "        return config_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "gather": {
     "logged": 1732887300975
    }
   },
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self,config:DataIngestionConfig):\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "    def initiate_data_ingestion(self):\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_timestamp = datetime.now().strftime(\"%m_%d_%Y %H:%M:%S\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            logging.info(f'Unziping file from {self.config.local_path} into {self.config.root_dir} ')\n",
    "            unzip_folder = os.path.join(self.config.root_dir,'unzipped_data') # folder to extract data to\n",
    "\n",
    "            if not os.path.exists(unzip_folder) or not os.listdir(unzip_folder) :\n",
    "                unzip_files(self.config.local_path,unzip_folder)\n",
    "                logging.info(f'Data from {self.config.local_path} unzippeded into {unzip_folder}')\n",
    "            else:\n",
    "                logging.info(f'data from {self.config.local_path} already unzipped')\n",
    "                pass\n",
    "\n",
    "\n",
    "            s3 = S3Client()\n",
    "            logging.info('Creating s3 bucket')\n",
    "            s3.create_bucket(bucketname='cosmetic-store-1')\n",
    "            logging.info('uploading files into s3 bucket')\n",
    "            s3.upload_folder(folder_path=unzip_folder,bucket=self.config.bucket,object_name_prefix=self.config.object_name_prefix)\n",
    "            logging.info('downloadin to local machine')\n",
    "            s3.download_folder(bucket=self.config.bucket,folder_prefix=self.config.object_name_prefix,local_dir=self.config.target_path)\n",
    "        \n",
    "\n",
    "\n",
    "            number_of_files = len(os.listdir(unzip_folder))\n",
    "            logging.info(f'Number of ingested files : {number_of_files}')\n",
    "\n",
    "\n",
    "            # save metadata\n",
    "            end_time = time.time()\n",
    "            end_timestamp = datetime.now().strftime(\"%m_%d_%Y %H:%M:%S\")\n",
    "            duration = end_time - start_time\n",
    "            metadata = {\n",
    "                'start_time' : start_timestamp,\n",
    "                'end_time' : end_timestamp,\n",
    "                'duration' : duration,\n",
    "                'Number of files loaded to workspace' : number_of_files,\n",
    "                'data_source' : self.config.local_path,\n",
    "                'target_path' : self.config.target_path,\n",
    "                'Project name' : 'Data ingestion'\n",
    "            }\n",
    "\n",
    "        \n",
    "            metadata_path = os.path.join(self.config.root_dir,'Data_ingestion_metadata.json')\n",
    "            pd.Series(metadata).to_json(metadata_path)\n",
    "    \n",
    "            logging.info(f'saved ingestion pipeline metadata into {metadata_path}')\n",
    "            \n",
    "        except ClientError as e:             \n",
    "            logging.info(f'error occured {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "gather": {
     "logged": 1732887778038
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-06 18:07:02,450 ] 39 root - INFO - Yaml file:  config\\config.yaml loaded suscessfully\n",
      "[2025-01-06 18:07:02,461 ] 39 root - INFO - Yaml file:  params.yaml loaded suscessfully\n",
      "[2025-01-06 18:07:02,523 ] 39 root - INFO - Yaml file:  schema.yaml loaded suscessfully\n",
      "[2025-01-06 18:07:02,536 ] 13 root - INFO - configuration: {'DATA_INGESTION': {'root_dir': 'Artifacts', 'bucket': 'cosmetic-store-1', 'object_name_prefix': 'customer_interation_data', 'local_path': 'Artifacts\\\\Ingested_data.zip', 'target_path': 'Artifacts\\\\ingested_data_files'}, 'DATA_VALIDATION': {'root_dir': 'Artifacts/data_validation', 'status_file': 'Artifacts/data_validation/status.json'}, 'DATA_TRANSFORMATION': {'source_datapath': 'Artifacts/Raw_ingested_data/*.csv', 'source_parquetpath': 'Artifacts/loaded_data.parquet', 'feature_store_path': 'Artifacts/FeatureStore', 'train_datapath': 'Artifacts/train_data', 'val_datapath': 'Artifacts/val_data', 'test_datapath': 'Artifacts/test_data'}}\n",
      "[2025-01-06 18:07:02,545 ] 62 root - INFO - File directory create at : Artifacts\n",
      "[2025-01-06 18:07:02,553 ] 13 root - INFO - Unziping file from Artifacts\\Ingested_data.zip into Artifacts \n",
      "[2025-01-06 18:07:02,564 ] 20 root - INFO - data from Artifacts\\Ingested_data.zip already unzipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-06 18:07:02,625 ] 25 root - INFO - Creating s3 bucket\n",
      "[2025-01-06 18:07:02,662 ] 39 root - INFO - Could not create bucket cosmetic-store-1 because An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n",
      "[2025-01-06 18:07:02,663 ] 27 root - INFO - uploading files into s3 bucket\n",
      "[2025-01-06 18:07:11,766 ] 89 root - INFO - Uploaded Artifacts\\unzipped_data\\2019-Dec.csv as customer_interation_data/2019-Dec.csv into cosmetic-store-1 S3 bucket\n",
      "[2025-01-06 18:07:24,883 ] 89 root - INFO - Uploaded Artifacts\\unzipped_data\\2019-Nov.csv as customer_interation_data/2019-Nov.csv into cosmetic-store-1 S3 bucket\n",
      "[2025-01-06 18:07:38,136 ] 89 root - INFO - Uploaded Artifacts\\unzipped_data\\2019-Oct.csv as customer_interation_data/2019-Oct.csv into cosmetic-store-1 S3 bucket\n",
      "[2025-01-06 18:07:52,766 ] 89 root - INFO - Uploaded Artifacts\\unzipped_data\\2020-Feb.csv as customer_interation_data/2020-Feb.csv into cosmetic-store-1 S3 bucket\n",
      "[2025-01-06 18:08:03,157 ] 89 root - INFO - Uploaded Artifacts\\unzipped_data\\2020-Jan.csv as customer_interation_data/2020-Jan.csv into cosmetic-store-1 S3 bucket\n",
      "[2025-01-06 18:08:03,240 ] 89 root - INFO - Uploaded Artifacts\\unzipped_data\\metadata.json as customer_interation_data/metadata.json into cosmetic-store-1 S3 bucket\n",
      "[2025-01-06 18:08:03,240 ] 29 root - INFO - downloadin to local machine\n",
      "[2025-01-06 18:08:08,791 ] 138 root - INFO - Downloaded customer_interation_data/2019-Dec.csv to Artifacts\\ingested_data_files\\2019-Dec.csv\n",
      "[2025-01-06 18:08:13,927 ] 138 root - INFO - Downloaded customer_interation_data/2019-Nov.csv to Artifacts\\ingested_data_files\\2019-Nov.csv\n",
      "[2025-01-06 18:08:19,099 ] 138 root - INFO - Downloaded customer_interation_data/2019-Oct.csv to Artifacts\\ingested_data_files\\2019-Oct.csv\n",
      "[2025-01-06 18:08:24,122 ] 138 root - INFO - Downloaded customer_interation_data/2020-Feb.csv to Artifacts\\ingested_data_files\\2020-Feb.csv\n",
      "[2025-01-06 18:08:29,481 ] 138 root - INFO - Downloaded customer_interation_data/2020-Jan.csv to Artifacts\\ingested_data_files\\2020-Jan.csv\n",
      "[2025-01-06 18:08:29,588 ] 138 root - INFO - Downloaded customer_interation_data/metadata.json to Artifacts\\ingested_data_files\\metadata.json\n",
      "[2025-01-06 18:08:29,592 ] 35 root - INFO - Number of ingested files : 6\n",
      "[2025-01-06 18:08:29,604 ] 56 root - INFO - saved ingestion pipeline metadata into Artifacts\\Data_ingestion_metadata.json\n"
     ]
    }
   ],
   "source": [
    "manager = ConfigurationManager()\n",
    "data_ingestion_config = manager.get_data_ingestion_config()\n",
    "\n",
    "\n",
    "data = DataIngestion(config=data_ingestion_config)\n",
    "filepath = data.initiate_data_ingestion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
